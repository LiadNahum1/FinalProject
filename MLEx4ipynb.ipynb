{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLEx4ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck7C7PF6Jy4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b882e9fa-2170-446e-bf60-1813adb072ce"
      },
      "source": [
        "######## required downloads and useful instructions ##############\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install biopython"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.7/dist-packages (1.79)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGRXtvfYiQjI"
      },
      "source": [
        "######### imports #############\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from skimage.transform import resize\n",
        "import tabulate\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, auc, roc_auc_score, average_precision_score\n",
        "from itertools import product\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from scipy.stats import friedmanchisquare\n",
        "from scipy.stats import f_oneway\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S83rTByglP8X"
      },
      "source": [
        "'''\n",
        "get all arguments, some are not changed by us and staded by the defult made by the aouthors of the paper.\n",
        "The ones we changed mostly are:\n",
        "  dataset - name of the database\n",
        "  epochs - amount of epochs\n",
        "  lr_init - the initial learning rate before momentum\n",
        "  swa  - whether to use swa or just make regular sgd\n",
        "  swa_start - from which ephocs to start the swa if swa is used\n",
        "  swa_lr - the learning rate that will be once starting the SWA - in the regular it will be the same as the lr_init and in our improvement it wll be 0.02 bigger\n",
        "  improved - whether to use swa improved \n",
        "'''\n",
        "def get_args():\n",
        "  parser = argparse.ArgumentParser(description='SGD/SWA/Improved training')\n",
        "  parser.add_argument('--dir', type=str, default='.', help='training directory')\n",
        "  parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset name')\n",
        "  parser.add_argument('--data_path', type=str, default='.', metavar='PATH',\n",
        "                      help='path to datasets location (default: .)')\n",
        "  parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size')\n",
        "  parser.add_argument('--num_workers', type=int, default=4, metavar='N', help='number of workers')\n",
        "\n",
        "\n",
        "  parser.add_argument('--epochs', type=int, default=200, metavar='N', help='number of epochs to train') \n",
        "  parser.add_argument('--lr_init', type=float, default=0.05, metavar='LR', help='initial learning rate')\n",
        "  parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='SGD momentum')\n",
        "  parser.add_argument('--wd', type=float, default=5e-4, help='weight decay')\n",
        "\n",
        "  parser.add_argument('--swa', action='store_true', default=False, help='swa usage flag (default: off)')\n",
        "  parser.add_argument('--swa_start', type=float, default=100, metavar='N', help='SWA start epoch number')\n",
        "  parser.add_argument('--swa_lr', type=float, default=0.07, metavar='LR', help='SWA LR')\n",
        "  parser.add_argument('--swa_c_epochs', type=int, default=1, metavar='N',\n",
        "                      help='SWA model collection frequency/cycle length in epochs')\n",
        "  parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed')\n",
        "\n",
        "  parser.add_argument('--improved', action='store_true', default=False, help='improved usage flag')\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQa6q6FUOEUh"
      },
      "source": [
        "'''\n",
        "VGG model definition, taken from the paper aouthors' repository \n",
        "ported from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "'''\n",
        "__all__ = ['VGG16']\n",
        "\n",
        "\n",
        "# reconstruct layers from model arhitecture \n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = list()\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "          conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "          if batch_norm:\n",
        "              layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "          else:\n",
        "              layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "          in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
        "         512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=10, depth=16, batch_norm=False):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = make_layers(cfg[depth], batch_norm)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Base:\n",
        "    base = VGG\n",
        "    args = list()\n",
        "    kwargs = dict()\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    #used for transforming gray scale image into RGB 32X32\n",
        "    transform_train_gray_scale = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Resize(32),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) ),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "       \n",
        "    ])\n",
        "\n",
        "    transform_test_gray_scale = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) ),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "       \n",
        "    ])  \n",
        "\n",
        "\n",
        "class VGG16(Base):\n",
        "    pass\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-iF8WstOPJN"
      },
      "source": [
        "def adjust_learning_rate(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "#update weigth average of the SWA model according to the paper\n",
        "def moving_average(net1, net2, alpha=1):\n",
        "    for param1, param2 in zip(net1.parameters(), net2.parameters()):\n",
        "        param1.data *= (1.0 - alpha)\n",
        "        param1.data += param2.data * alpha\n",
        "\n",
        "#constant learning rate schedule\n",
        "def schedule(epoch, lr_init, num_epoch):\n",
        "    t = (epoch) / (args.swa_start if args.swa else num_epoch)\n",
        "    lr_ratio = args.swa_lr / lr_init if args.swa else 0.01\n",
        "    if t <= 0.5:\n",
        "        factor = 1.0\n",
        "    elif t <= 0.9:\n",
        "        factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
        "    else:\n",
        "        factor = lr_ratio\n",
        "    return lr_init * factor\n",
        "\n",
        "\n",
        "# trains epoch using the VGG16 model and returns accuracy and loss metrics\n",
        "def train_epoch(loader, model, criterion, optimizer):\n",
        "    loss_sum = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (input, target) in enumerate(loader):\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * input.size(0)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target_var.data.view_as(pred)).sum().item()\n",
        "\n",
        "    return {\n",
        "        'loss': loss_sum / len(loader.dataset),\n",
        "        'accuracy': correct / len(loader.dataset) * 100.0,\n",
        "    }\n",
        "\n",
        "def get_metrics_one_vs_rest( y_true, predicted, proba):\n",
        "  cm =  confusion_matrix(y_true, predicted)\n",
        "  TN = cm[0][0]\n",
        "  FP = cm[0][1]\n",
        "  FN = cm[1][0]\n",
        "  TP = cm[1][1] \n",
        "  # Overall accuracy for each class\n",
        "  ACC = (TP+TN)/(TP+FP+FN+TN) # doesn't calculate as in the paper\n",
        "  # true positive rate\n",
        "  TPR = TP/(TP+FN)\n",
        "  # false positive rate\n",
        "  FPR = FP/(FP+TN)\n",
        "  # Precision or positive predictive value\n",
        "  Precision = TP/(TP+FP)\n",
        "  #AUC\n",
        "  AUC = roc_auc_score(y_true, proba)\n",
        "  #PR-CURVE\n",
        "  PR_CURVE = average_precision_score(y_true, proba)\n",
        "  return { 'ACC': ACC, 'TPR': TPR, 'FPR':FPR, 'Precision': Precision, 'AUC':AUC, 'PR_CURVE':PR_CURVE}   \n",
        "\n",
        "'''\n",
        "calculates all metrics in a one vs rest approach.\n",
        "returns dictionary of ACC, TPR, FPR, Precision, AUC, PR-CURVE values\n",
        "'''\n",
        "def one_vs_rest(y_true, predicted, proba, num_classes):\n",
        "  metrics_all_classes = []\n",
        "  for class_num in range(num_classes):\n",
        "    y_true_class = [1 if y==class_num else 0 for y in y_true]\n",
        "    predicted_class = [1 if pred==class_num else 0 for pred in predicted]\n",
        "    one_class_values = list(get_metrics_one_vs_rest(y_true_class, predicted_class, proba[:, class_num]).values())\n",
        "    metrics_all_classes.append(one_class_values)\n",
        "  metrics_all_classes = np.mean(metrics_all_classes, axis=0)\n",
        "  metrics =['ACC', 'TPR', 'FPR', 'Precision', 'AUC', 'PR_CURVE']\n",
        "  metrics_results = dict()\n",
        "  for i in range(len(metrics)):\n",
        "    metrics_results[metrics[i]] = metrics_all_classes[i]\n",
        "  return metrics_results\n",
        "\n",
        "\n",
        "'''\n",
        "evaluates the model and returns loss, accuracy and metrics dictionary which includes the following metrics:\n",
        "Accuracy, TPR, FPR, Precision, AUC, Area under the Precision-Recall, Training time\n",
        "and Inference time for 1000 instances\n",
        "'''\n",
        "def eval(loader, model, criterion, num_classes, training_time=0):\n",
        "    loss_sum = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    y_true = torch.tensor([], dtype=torch.long, device=torch.device('cuda:0'))\n",
        "    predicted = torch.tensor([], device=torch.device('cuda:0'))\n",
        "    proba = torch.tensor([], device=torch.device('cuda:0'))\n",
        "\n",
        "    inference_time = time.time()\n",
        "    inference_times = []\n",
        "    for i, (input, target) in enumerate(loader):\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "        loss_sum += loss.item() * input.size(0)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target_var.data.view_as(pred)).sum().item()\n",
        "\n",
        "        y_true = torch.cat((y_true, target_var), 0) #true lables\n",
        "        predicted = torch.cat((predicted, output.data.max(1)[1]), 0) #predicted labels\n",
        "        proba = torch.cat((proba, output.data), 0) #probabilites of certainty\n",
        "\n",
        "        if (i + 1) % 8 == 0: #for calculating evaluation time for 1000 instances \n",
        "          inference_time = time.time() - inference_time \n",
        "          inference_times.append(inference_time)\n",
        "          inference_time = time.time()\n",
        "\n",
        "    predicted = predicted.cpu().numpy()\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    proba = proba.cpu().numpy()\n",
        "    \n",
        "    #calculate all metrics using one vs rest approach \n",
        "    metrics = one_vs_rest(y_true, predicted, proba, num_classes)\n",
        "    metrics['Training Time'] = training_time\n",
        "    metrics['Inference Time'] =  np.mean(inference_times)\n",
        "\n",
        "    accuracy =  correct / len(loader.dataset) * 100.0\n",
        "    return {\n",
        "        'loss': loss_sum / len(loader.dataset),\n",
        "        'accuracy': accuracy,\n",
        "        'metrics': metrics,\n",
        "    }\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6LgUU3wgKsb"
      },
      "source": [
        "'''\n",
        "The function is activated for hyper-parameters optimization only.\n",
        "Gets hyper-parameters configuration and returns accuracy value.\n",
        "'''\n",
        "def find_accuracy_for_specific_hyperparameters(config , model_cfg, loaders):      \n",
        "  print('Find best parameters')\n",
        "  model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
        "  model.cuda()\n",
        "\n",
        "  num_epoch = config[1]\n",
        "  lr_init = config[0]\n",
        "\n",
        "  criterion = F.cross_entropy\n",
        "  optimizer = torch.optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=lr_init,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.wd\n",
        "  )\n",
        "  start_epoch = 0\n",
        "\n",
        "  for epoch in range(start_epoch, num_epoch):\n",
        "      lr = schedule(epoch, lr_init, num_epoch)\n",
        "      adjust_learning_rate(optimizer, lr)\n",
        "      train_res = train_epoch(loaders['train'], model, criterion, optimizer)\n",
        "      test_res = eval(loaders['val'], model, criterion, num_classes)\n",
        "\n",
        "      #printing per epoch \n",
        "      values = [epoch + 1, lr, train_res['loss'], train_res['accuracy'], test_res['loss'], test_res['accuracy']]\n",
        "      table = tabulate.tabulate([values], ['ep', 'lr', 'tr_loss', 'tr_acc', 'te_loss', 'te_acc'], tablefmt='simple', floatfmt='8.4f')\n",
        "      print(table)\n",
        "\n",
        "  return test_res['accuracy'] "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhxTp4c1iiES"
      },
      "source": [
        "'''\n",
        "The function evaluates the model. \n",
        "Applies conventional SGD or SWA algorithms according to argument args.swa\n",
        "'''\n",
        "def evaluate_model(lr_init, num_epoch, train_loader, test_loader, num_classes):      \n",
        "  print('Evaluate model')\n",
        "  model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
        "  model.cuda()\n",
        "\n",
        "  if args.swa:\n",
        "      swa_model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
        "      swa_model.cuda()\n",
        "      swa_n = 0\n",
        "\n",
        "  criterion = F.cross_entropy\n",
        "  optimizer = torch.optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=lr_init,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.wd\n",
        "  )\n",
        "  start_epoch = 0\n",
        "  \n",
        "  for epoch in range(start_epoch, num_epoch):\n",
        "      lr = schedule(epoch, lr_init, num_epoch)\n",
        "      adjust_learning_rate(optimizer, lr)\n",
        "      training_time = time.time()\n",
        "      train_res = train_epoch(train_loader, model, criterion, optimizer)\n",
        "      training_time = time.time() - training_time\n",
        "      test_res = eval(test_loader, model, criterion, num_classes, training_time)\n",
        "\n",
        "      if not args.swa:\n",
        "         print(f'{test_res},')\n",
        "\n",
        "      #if we are using swa and we are at the swa phase and (we are using constant learning rate schedule or we are at a start of a cycle in a cyclic schedule)\n",
        "      if args.swa and (epoch + 1) >= args.swa_start and (epoch + 1 - args.swa_start) % args.swa_c_epochs == 0:\n",
        "          # calculate the avarage of the weights\n",
        "          moving_average(swa_model, model, 1.0 / (swa_n + 1))\n",
        "          swa_n += 1\n",
        "          # evaluate test-set performance \n",
        "          swa_res = eval(test_loader, swa_model, criterion, num_classes, training_time)\n",
        "          print(f'{swa_res},')  \n",
        "          \n",
        "      elif args.swa:\n",
        "        print(f'epoch = {epoch}/{num_epoch},')       \n",
        "      \n",
        "  if args.swa:\n",
        "    return swa_res\n",
        "  else:\n",
        "    return test_res"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiN6t4Oxfs3u"
      },
      "source": [
        "def hyper_parameters_optimization(dataset):\n",
        "  if args.swa:  # optimization is applied only on sgd \n",
        "      return [args.lr_init, args.epochs]\n",
        "  best_parameters = None\n",
        "  best_accuracy = 0\n",
        "  # outer k-fold Cross Validation \n",
        "  kfold = KFold(n_splits=10, shuffle=True)\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "      train_subs = torch.utils.data.dataset.Subset(dataset, train_ids)\n",
        "      #hyper-parameters space\n",
        "      config = {\n",
        "          'lr_init': [x / 100.0 for x in range(1, 11)],\n",
        "          'num_epochs': list(range(50, 300, 25))\n",
        "      }\n",
        "      all_options = list(product(config['lr_init'], config['num_epochs']))\n",
        "      # select randomly 50 times\n",
        "      for _ in range(50):\n",
        "          curr_config = random.choice(all_options)\n",
        "          print(f'choice {curr_config}')\n",
        "          # inner k-fold for hyperparameters optimization\n",
        "          mean_acc = 0\n",
        "          inner_kfold = KFold(n_splits=3, shuffle=True)\n",
        "          for fold, (in_train_ids, in_val_ids) in enumerate(inner_kfold.split(train_subs)):\n",
        "              inner_train = torch.utils.data.dataset.Subset(dataset, in_train_ids)\n",
        "              inner_val = torch.utils.data.dataset.Subset(dataset, in_val_ids)\n",
        "\n",
        "              # Define data loaders for training and validation data in this fold\n",
        "              inner_train_loader = torch.utils.data.DataLoader(\n",
        "                  inner_train,\n",
        "                  batch_size=args.batch_size,\n",
        "                  num_workers=args.num_workers,\n",
        "                  shuffle=True)\n",
        "\n",
        "              inner_val_loader = torch.utils.data.DataLoader(\n",
        "                  inner_val,\n",
        "                  batch_size=args.batch_size,\n",
        "                  shuffle=False,\n",
        "                  num_workers=args.num_workers)\n",
        "\n",
        "              loaders = {'train': inner_train_loader, 'val': inner_val_loader}\n",
        "              # train the network with the chosen hyperparameters\n",
        "              acc = find_accuracy_for_specific_hyperparameters(curr_config, model_cfg, loaders)\n",
        "              mean_acc += acc\n",
        "\n",
        "          # determine the mean accurecy to be mean_acc / 3\n",
        "          mean_acc = mean_acc / 3\n",
        "          if best_accuracy < mean_acc:\n",
        "              best_accuracy = mean_acc\n",
        "              # choose the best to be the one with highest mean accrucy over the 3 inner folds.\n",
        "              best_parameters = curr_config\n",
        "          print(f'best parameters {best_parameters}')\n",
        "\n",
        "  return best_parameters "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2yYszCkrV3"
      },
      "source": [
        "'''\n",
        "Downloads the desired dataset according to args.dataset, applies transformation if needed, determined number of classes\n",
        "'''\n",
        "def download_dataset():\n",
        "  if args.dataset == 'EMNIST':\n",
        "    train_set = ds(path, train=True, download=True, transform=model_cfg.transform_train_gray_scale, split='digits')\n",
        "    test_set = ds(path, train=False, download=True, transform=model_cfg.transform_test_gray_scale, split='digits')\n",
        "  elif args.dataset == 'MNIST' or args.dataset == 'USPS' or args.dataset == 'FashionMNIST' or args.dataset == 'KMNIST' or args.dataset == 'QMNIST':\n",
        "    train_set = ds(path, train=True, download=True, transform=model_cfg.transform_train_gray_scale)\n",
        "    test_set = ds(path, train=False, download=True, transform=model_cfg.transform_test_gray_scale)\n",
        "  elif args.dataset == 'STL10' or args.dataset == 'SVHN':\n",
        "    train_set = ds(path, download= True ,transform=model_cfg.transform_train)\n",
        "  elif args.dataset == 'SEMEION':\n",
        "     train_set = ds(path, download=True, transform=model_cfg.transform_train_gray_scale)\n",
        "  else:# CIFAR10, CIFAR100\n",
        "    train_set = ds(path, train=True, download=True, transform=model_cfg.transform_train)\n",
        "    test_set = ds(path, train=False, download=True, transform=model_cfg.transform_test)\n",
        "\n",
        "  if args.dataset == 'CIFAR100':\n",
        "    num_classes = 100 \n",
        "  else:\n",
        "    num_classes = 10 \n",
        "  if args.dataset == 'STL10' or args.dataset == 'SVHN' or args.dataset == 'SEMEION':\n",
        "    dataset = train_set\n",
        "  else:\n",
        "    dataset = ConcatDataset([train_set, test_set])\n",
        "  return dataset, num_classes"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFbws6EvhmZb"
      },
      "source": [
        "def check_if_nan(value):\n",
        "  for val_ind in range(len(value))[4:]:\n",
        "      if math.isnan(value[val_ind]):\n",
        "        value[val_ind] = 0 \n",
        "  return value"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR84jyx4mL4J"
      },
      "source": [
        "'''\n",
        "Gets three models and return whether they have the same distribution\n",
        "'''\n",
        "def friedman_test(sgd, swa, improved):\n",
        "  # compare samples\n",
        "  return friedmanchisquare(sgd, swa, improved)\n",
        "\n",
        "'''\n",
        "Get three models and calculates the differences between them in terms of p-value and mean difference\n",
        "'''\n",
        "def post_hoc_test(sgd, swa, improved):\n",
        "  scores = sgd\n",
        "  scores.extend(swa)\n",
        "  scores.extend(improved)\n",
        "  #create DataFrame to hold data\n",
        "  df = pd.DataFrame({'score': scores,\n",
        "                    'group': np.repeat(['SGD', 'SWA', 'Improved'], repeats=10)}) \n",
        "  # perform Tukey's test\n",
        "  tukey = pairwise_tukeyhsd(endog=df['score'], groups=df['group'], alpha=0.05)\n",
        "  #display results\n",
        "  print(tukey)\n",
        "\n",
        "'''\n",
        "Get the file name of the file with the results of the 3 algorithms,\n",
        "the wanted dataset and the wanted metric on which we want to test if the preformances were the same\n",
        "and prints the results.\n",
        "'''\n",
        "def significant_test(file_name, dataset_name, metric):\n",
        "  # open the xlsx file with the results of the datasets\n",
        "  df = pd.read_excel(file_name, header=None).iloc[:,:-1]\n",
        "  df.columns= ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "  # take the results of the metric in the dataset for each algorithm\n",
        "  sgd = df.loc[(df['Dataset Name'] == dataset_name) & (df['Algorithm Name'] == 'SGD')][metric].values.tolist()\n",
        "  swa = df.loc[(df['Dataset Name'] == dataset_name) & (df['Algorithm Name'] == 'SWA')][metric].values.tolist()\n",
        "  improved =  df.loc[(df['Dataset Name'] == dataset_name) & (df['Algorithm Name'] == 'improved')][metric].values.tolist()\n",
        "  # apply the firdeman test\n",
        "  stat, p = friedman_test(sgd, swa, improved)\n",
        "  \n",
        "  # interpret - if the distributions are different then apply also post_hoc test to measure the difference\n",
        "  alpha = 0.05\n",
        "  if p > alpha:\n",
        "    print('Same distributions (fail to reject H0)')\n",
        "  else:\n",
        "    print('Different distributions (reject H0)')\n",
        "    post_hoc_test(sgd, swa, improved)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BTJUlHxy3Vd"
      },
      "source": [
        "\"\"\"\n",
        "  MAIN\n",
        "\"\"\"\n",
        "#parse args\n",
        "args = get_args()\n",
        "\n",
        "# set up directory\n",
        "os.makedirs(args.dir, exist_ok=True)\n",
        "with open(os.path.join(args.dir, 'command.sh'), 'w') as f:\n",
        "    f.write(' '.join(sys.argv))\n",
        "    f.write('\\n')\n",
        "\n",
        "# set up cuda with seed\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# set up model \n",
        "model_cfg = VGG16()  \n",
        "\n",
        "#chose the algorithm to run\n",
        "if args.improved:\n",
        "  print('Using SWA Improved')\n",
        "  alg_name = 'improved'\n",
        "elif args.swa:\n",
        "  print('Using SWA')\n",
        "  alg_name = 'SWA'\n",
        "else:\n",
        "  print('Using SGD')\n",
        "  alg_name = 'SGD'\n",
        "\n",
        "#loading dataset from torchvision package \n",
        "print('Loading dataset %s from %s' % (args.dataset, args.data_path))\n",
        "ds = getattr(torchvision.datasets, args.dataset)  \n",
        "path = os.path.join(args.data_path, args.dataset.lower())\n",
        "\n",
        "dataset, num_classes = download_dataset()\n",
        "\n",
        "\n",
        "values = []\n",
        "columns = ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "\n",
        "#choose the best parameters by the hyper parameters optimization\n",
        "best_parameters = hyper_parameters_optimization(dataset)\n",
        "\n",
        "# outer k-fold Cross Validation \n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    train_subs = torch.utils.data.dataset.Subset(dataset, train_ids)\n",
        "    test_subs = torch.utils.data.dataset.Subset(dataset, test_ids)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    train_subs,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=args.num_workers,\n",
        "    shuffle=True)\n",
        "        \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    test_subs,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=args.num_workers,\n",
        "    shuffle=True)\n",
        "\n",
        "    # train the model with the best parameters chosen and evaluate this results on the K outer datasets\n",
        "    res = evaluate_model(best_parameters[0], best_parameters[1], train_loader, test_loader, num_classes)\n",
        "    value = [args.dataset, alg_name, fold+1, best_parameters] + list(res['metrics'].values())\n",
        "    value = check_if_nan(value)\n",
        "    values.append(value)\n",
        "\n",
        "# put the reults into csv file. later,  for the submission and statistics tests we transformed it to an arranged xlsx file !!!\n",
        "df_results = pd.DataFrame(data=values, columns = columns)\n",
        "if os.path.isfile('results.csv'):\n",
        "  df_results.to_csv('results.csv', mode='a', index=False)\n",
        "else:\n",
        "  df_results.to_csv('results.csv', index=False)\n",
        "print(df_results.to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLfBiuRAi7D_",
        "outputId": "98b3c9f7-f6b5-4720-fed4-e2a7e7a588c9"
      },
      "source": [
        "# run the significant test for the datasets we have the results for\n",
        "for name in ['CIFAR10', 'CIFAR100', 'EMNIST', 'FashionMNIST', 'KMNIST','MNIST', 'QMNIST', 'STL10', 'USPS', 'SVHN', 'SEMEION']:\n",
        "  print(f'Dataset {name}')\n",
        "  significant_test('results.xlsx',name, 'ACC')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset CIFAR10\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj  lower   upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD   0.0061  0.001 0.0056  0.0066   True\n",
            "Improved    SWA   0.0055  0.001  0.005   0.006   True\n",
            "     SGD    SWA  -0.0005 0.0266 -0.001 -0.0001   True\n",
            "-----------------------------------------------------\n",
            "Dataset CIFAR100\n",
            "Different distributions (reject H0)\n",
            "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
            "===================================================\n",
            " group1  group2 meandiff p-adj lower  upper  reject\n",
            "---------------------------------------------------\n",
            "Improved    SGD   0.0071 0.001 0.0067 0.0074   True\n",
            "Improved    SWA   0.0092 0.001 0.0088 0.0095   True\n",
            "     SGD    SWA   0.0021 0.001 0.0018 0.0024   True\n",
            "---------------------------------------------------\n",
            "Dataset EMNIST\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
            "======================================================\n",
            " group1  group2 meandiff p-adj   lower   upper  reject\n",
            "------------------------------------------------------\n",
            "Improved    SGD  -0.0002  0.001 -0.0003 -0.0001   True\n",
            "Improved    SWA  -0.0001 0.0139 -0.0002    -0.0   True\n",
            "     SGD    SWA   0.0001  0.001     0.0  0.0002   True\n",
            "------------------------------------------------------\n",
            "Dataset FashionMNIST\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj   lower  upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD   0.0019  0.001  0.0016 0.0022   True\n",
            "Improved    SWA   0.0015  0.001  0.0012 0.0019   True\n",
            "     SGD    SWA  -0.0004 0.0305 -0.0007   -0.0   True\n",
            "-----------------------------------------------------\n",
            "Dataset KMNIST\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj  lower   upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD   -0.003 0.001 -0.0036 -0.0024   True\n",
            "Improved    SWA  -0.0001   0.9 -0.0007  0.0005  False\n",
            "     SGD    SWA   0.0029 0.001  0.0023  0.0035   True\n",
            "-----------------------------------------------------\n",
            "Dataset MNIST\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj  lower   upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD   0.0003 0.001  0.0002  0.0003   True\n",
            "Improved    SWA  -0.0004 0.001 -0.0004 -0.0003   True\n",
            "     SGD    SWA  -0.0006 0.001 -0.0007 -0.0006   True\n",
            "-----------------------------------------------------\n",
            "Dataset QMNIST\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
            "======================================================\n",
            " group1  group2 meandiff p-adj   lower   upper  reject\n",
            "------------------------------------------------------\n",
            "Improved    SGD  -0.0008  0.001  -0.001 -0.0006   True\n",
            "Improved    SWA  -0.0001 0.7024 -0.0003  0.0001  False\n",
            "     SGD    SWA   0.0007  0.001  0.0005  0.0009   True\n",
            "------------------------------------------------------\n",
            "Dataset STL10\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj  lower   upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD   0.0294 0.001  0.0253  0.0334   True\n",
            "Improved    SWA     0.01 0.001  0.0059   0.014   True\n",
            "     SGD    SWA  -0.0194 0.001 -0.0235 -0.0153   True\n",
            "-----------------------------------------------------\n",
            "Dataset USPS\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj  lower   upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD  -0.0011 0.001 -0.0016 -0.0007   True\n",
            "Improved    SWA  -0.0129 0.001 -0.0133 -0.0124   True\n",
            "     SGD    SWA  -0.0118 0.001 -0.0122 -0.0113   True\n",
            "-----------------------------------------------------\n",
            "Dataset SVHN\n",
            "Different distributions (reject H0)\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
            "=====================================================\n",
            " group1  group2 meandiff p-adj  lower   upper  reject\n",
            "-----------------------------------------------------\n",
            "Improved    SGD  -0.0017  0.001 -0.002 -0.0015   True\n",
            "Improved    SWA   0.0004 0.0029 0.0001  0.0007   True\n",
            "     SGD    SWA   0.0022  0.001 0.0019  0.0024   True\n",
            "-----------------------------------------------------\n",
            "Dataset SEMEION\n",
            "Same distributions (fail to reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}