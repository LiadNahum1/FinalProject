{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLEx4ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck7C7PF6Jy4_"
      },
      "source": [
        "######## required downloads and useful instructions ##############\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install ray torch torchvision\n",
        "!pip install 'ray[tune]'\n",
        "!pip install biopython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGRXtvfYiQjI"
      },
      "source": [
        "######### imports #############\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from skimage.transform import resize\n",
        "import tabulate\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, auc, roc_auc_score, average_precision_score\n",
        "from itertools import product\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from scipy.stats import friedmanchisquare\n",
        "from scipy.stats import f_oneway\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S83rTByglP8X"
      },
      "source": [
        "def get_args():\n",
        "  parser = argparse.ArgumentParser(description='SGD/SWA training')\n",
        "  parser.add_argument('--dir', type=str, default='.', help='training directory (default: None)')\n",
        "\n",
        "  parser.add_argument('--dataset', type=str, default='SEMEION', help='dataset name (default: CIFAR10)')\n",
        "  parser.add_argument('--data_path', type=str, default='.', metavar='PATH',\n",
        "                      help='path to datasets location (default: None)')\n",
        "  parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size (default: 128)')\n",
        "  parser.add_argument('--num_workers', type=int, default=4, metavar='N', help='number of workers (default: 4)')\n",
        "  parser.add_argument('--model', type=str, default='VGG16', metavar='MODEL',\n",
        "                      help='model name (default: None)')\n",
        "\n",
        "  parser.add_argument('--resume', type=str, default=None, metavar='CKPT',\n",
        "                      help='checkpoint to resume training from (default: None)')\n",
        "\n",
        "  parser.add_argument('--epochs', type=int, default=200, metavar='N', help='number of epochs to train (default: 200)')\n",
        "  parser.add_argument('--save_freq', type=int, default=25, metavar='N', help='save frequency (default: 25)')\n",
        "  parser.add_argument('--eval_freq', type=int, default=5, metavar='N', help='evaluation frequency (default: 5)')\n",
        "  parser.add_argument('--lr_init', type=float, default=0.05, metavar='LR', help='initial learning rate (default: 0.01)')\n",
        "  parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='SGD momentum (default: 0.9)')\n",
        "  parser.add_argument('--wd', type=float, default=5e-4, help='weight decay (default: 1e-4)')\n",
        "\n",
        "  parser.add_argument('--swa', action='store_true', default=False, help='swa usage flag (default: off)')\n",
        "  parser.add_argument('--swa_start', type=float, default=100, metavar='N', help='SWA start epoch number (default: 161)')\n",
        "  parser.add_argument('--swa_lr', type=float, default=0.07, metavar='LR', help='SWA LR (default: 0.05)')\n",
        "  parser.add_argument('--swa_c_epochs', type=int, default=1, metavar='N',\n",
        "                      help='SWA model collection frequency/cycle length in epochs (default: 1)')\n",
        "  parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 1)')\n",
        "\n",
        "  parser.add_argument('--imrpoved', action='store_true', default=True, help='improved usage flag (default: off)')\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args "
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQa6q6FUOEUh"
      },
      "source": [
        "\"\"\"\n",
        "    VGG model definition\n",
        "    ported from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "\"\"\"\n",
        "\n",
        "# possible models for net architecture\n",
        "__all__ = ['VGG16']\n",
        "\n",
        "\n",
        "# reconstruct layers from model arhitecture \n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = list()\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "          conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "          if batch_norm:\n",
        "              layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "          else:\n",
        "              layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "          in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
        "         512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=10, depth=16, batch_norm=False):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = make_layers(cfg[depth], batch_norm)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Base:\n",
        "    base = VGG\n",
        "    args = list()\n",
        "    kwargs = dict()\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    transform_train_gray_scale = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Resize(32),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) ),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "       \n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    \n",
        "    transform_test_gray_scale = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) ),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "       \n",
        "    ])  \n",
        "\n",
        "\n",
        "class VGG16(Base):\n",
        "    pass\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-iF8WstOPJN"
      },
      "source": [
        "\"\"\"\n",
        "    utils.py\n",
        "\"\"\"\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def train_epoch(loader, model, criterion, optimizer):\n",
        "    loss_sum = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (input, target) in enumerate(loader):\n",
        "        input = input.cuda()#async=True)\n",
        "        target = target.cuda()#sasync=True)\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * input.size(0)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target_var.data.view_as(pred)).sum().item()\n",
        "\n",
        "    return {\n",
        "        'loss': loss_sum / len(loader.dataset),\n",
        "        'accuracy': correct / len(loader.dataset) * 100.0,\n",
        "    }\n",
        "\n",
        "def one_vs_rest(y_true, predicted, proba, num_classes):\n",
        "  metrics_all_classes = []\n",
        "  for class_num in range(num_classes):\n",
        "    y_true_class = [1 if y==class_num else 0 for y in y_true]\n",
        "    predicted_class = [1 if pred==class_num else 0 for pred in predicted]\n",
        "    metrics_all_classes.append(list(get_metrics_one_vs_rest(y_true_class, predicted_class, proba[:, class_num]).values()))\n",
        "  metrics_all_classes = np.mean(metrics_all_classes, axis=0)\n",
        "  metrics =['ACC', 'TPR', 'FPR', 'Precision', 'AUC', 'PR_CURVE']\n",
        "  metrics_results = dict()\n",
        "  for i in range(len(metrics)):\n",
        "    metrics_results[metrics[i]] = metrics_all_classes[i]\n",
        "  return metrics_results\n",
        "\n",
        "\n",
        "def get_metrics_one_vs_rest( y_true, predicted, proba):\n",
        "  cm =  confusion_matrix(y_true, predicted)\n",
        "  TN = cm[0][0]\n",
        "  FP = cm[0][1]\n",
        "  FN = cm[1][0]\n",
        "  TP = cm[1][1] \n",
        "  # Overall accuracy for each class\n",
        "  ACC = (TP+TN)/(TP+FP+FN+TN) # doesn't calculate as in the paper\n",
        "  # true positive rate\n",
        "  TPR = TP/(TP+FN)\n",
        "  # false positive rate\n",
        "  FPR = FP/(FP+TN)\n",
        "  # Precision or positive predictive value\n",
        "  Precision = TP/(TP+FP)\n",
        "  #AUC\n",
        "  AUC = roc_auc_score(y_true, proba)\n",
        "  #PR-CURVE\n",
        "  PR_CURVE = average_precision_score(y_true, proba)\n",
        "  return { 'ACC': ACC, 'TPR': TPR, 'FPR':FPR, 'Precision': Precision, 'AUC':AUC, 'PR_CURVE':PR_CURVE}\n",
        "\n",
        "def eval(loader, model, criterion, num_classes, training_time=0):\n",
        "    loss_sum = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    y_true = torch.tensor([], dtype=torch.long, device=torch.device('cuda:0'))\n",
        "    predicted = torch.tensor([], device=torch.device('cuda:0'))\n",
        "    proba = torch.tensor([], device=torch.device('cuda:0'))\n",
        "\n",
        "    inference_time = time.time()\n",
        "    inference_times = []\n",
        "    for i, (input, target) in enumerate(loader):\n",
        "        input = input.cuda()#async=True)\n",
        "        target = target.cuda()#async=True)\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "        #print(f'input{input}\\n input var{input_var}\\n target{target}\\n target_var{target_var}')\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "        loss_sum += loss.item() * input.size(0)\n",
        "        #print(f'data {output.data} \\n data max { output.data.max(1)} \\n pred {output.data.max(1, keepdim=True)[1]} pred {output.data.max(1)[1]}')\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target_var.data.view_as(pred)).sum().item()\n",
        "        #print(f'target as pred {target_var.data.view_as(pred)}\\n sum {pred.eq(target_var.data.view_as(pred)).sum()}\\n item {pred.eq(target_var.data.view_as(pred)).sum().item()}')\n",
        "        y_true = torch.cat((y_true, target_var), 0)\n",
        "        predicted = torch.cat((predicted, output.data.max(1)[1]), 0)\n",
        "        proba = torch.cat((proba, output.data), 0)\n",
        "        if (i + 1) % 8 == 0:\n",
        "          inference_time = time.time() - inference_time \n",
        "          inference_times.append(inference_time)\n",
        "          inference_time = time.time()\n",
        "\n",
        "    predicted = predicted.cpu().numpy()\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    proba = proba.cpu().numpy()\n",
        "    \n",
        "    metrics = one_vs_rest(y_true, predicted, proba, num_classes)\n",
        "    metrics['Training Time'] = training_time\n",
        "    metrics['Inference Time'] =  np.mean(inference_times)\n",
        "    accuracy =  correct / len(loader.dataset) * 100.0\n",
        "    return {\n",
        "        'loss': loss_sum / len(loader.dataset),\n",
        "        'accuracy': accuracy,\n",
        "        'metrics': metrics,\n",
        "    }\n",
        "\n",
        "\n",
        "def moving_average(net1, net2, alpha=1):\n",
        "    for param1, param2 in zip(net1.parameters(), net2.parameters()):\n",
        "        param1.data *= (1.0 - alpha)\n",
        "        param1.data += param2.data * alpha\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6LgUU3wgKsb"
      },
      "source": [
        "def find_accuracy_for_specific_hyperparameters(config , model_cfg, loaders):      \n",
        "  print('Find best parameters')\n",
        "  model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
        "  model.cuda()\n",
        "\n",
        "  num_epoch = config[1]\n",
        "  lr_init = config[0]\n",
        "\n",
        "  criterion = F.cross_entropy\n",
        "  optimizer = torch.optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=lr_init,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.wd\n",
        "  )\n",
        "  start_epoch = 0\n",
        "\n",
        "  for epoch in range(start_epoch, num_epoch)[:1]:\n",
        "      lr = schedule(epoch, lr_init, num_epoch)\n",
        "      # utils.\n",
        "      adjust_learning_rate(optimizer, lr)\n",
        "      train_res = train_epoch(loaders['train'], model, criterion, optimizer)\n",
        "      test_res = eval(loaders['val'], model, criterion, num_classes)\n",
        "\n",
        "      #printing per epoch \n",
        "      values = [epoch + 1, lr, train_res['loss'], train_res['accuracy'], test_res['loss'], test_res['accuracy']]\n",
        "      table = tabulate.tabulate([values], ['ep', 'lr', 'tr_loss', 'tr_acc', 'te_loss', 'te_acc'], tablefmt='simple', floatfmt='8.4f')\n",
        "      print(table)\n",
        "\n",
        "  return test_res['accuracy'] "
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhxTp4c1iiES"
      },
      "source": [
        "def evaluate_model(lr_init, num_epoch, train_loader, test_loader, num_classes):      \n",
        "  print('Evaluate model')\n",
        "  model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
        "  model.cuda()\n",
        "\n",
        "  if args.swa:\n",
        "      swa_model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
        "      swa_model.cuda()\n",
        "      swa_n = 0\n",
        "\n",
        "  criterion = F.cross_entropy\n",
        "  optimizer = torch.optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=lr_init,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.wd\n",
        "  )\n",
        "  start_epoch = 0\n",
        "  \n",
        "  for epoch in range(start_epoch, num_epoch):\n",
        "      lr = schedule(epoch, lr_init, num_epoch)\n",
        "      # utils.\n",
        "      adjust_learning_rate(optimizer, lr)\n",
        "      training_time = time.time()\n",
        "      train_res = train_epoch(train_loader, model, criterion, optimizer)\n",
        "      training_time = time.time() - training_time\n",
        "      test_res = eval(test_loader, model, criterion, num_classes, training_time)\n",
        "\n",
        "      if not args.swa:\n",
        "         print(f'{test_res},')\n",
        "\n",
        "      #if we are using swa and we are in the part of the swa and the modulu of the cycle is 0:\n",
        "      if args.swa and (epoch + 1) >= args.swa_start and (epoch + 1 - args.swa_start) % args.swa_c_epochs == 0:\n",
        "          # calculate the avarage of the weights\n",
        "          moving_average(swa_model, model, 1.0 / (swa_n + 1))\n",
        "          swa_n += 1\n",
        "          # evaluate test preformantce with the parameters\n",
        "          swa_res = eval(test_loader, swa_model, criterion, num_classes, training_time)  \n",
        "          print(f'{swa_res},')\n",
        "      elif args.swa:\n",
        "        print(f'epoch = {epoch}/{num_epoch},')       \n",
        "      \n",
        "  if args.swa:\n",
        "    return swa_res\n",
        "  else:\n",
        "    return test_res"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsVTI1ZLhXpc"
      },
      "source": [
        "def schedule(epoch, lr_init, num_epoch):\n",
        "    t = (epoch) / (args.swa_start if args.swa else num_epoch)\n",
        "    lr_ratio = args.swa_lr / lr_init if args.swa else 0.01\n",
        "    if t <= 0.5:\n",
        "        factor = 1.0\n",
        "    elif t <= 0.9:\n",
        "        factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
        "    else:\n",
        "        factor = lr_ratio\n",
        "    return lr_init * factor\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiN6t4Oxfs3u"
      },
      "source": [
        "def hyper_parameters_optimization(dataset):\n",
        "  if args.swa:  # optimization is applied only on sgd \n",
        "      return [args.lr_init, args.epochs]\n",
        "  best_parameters = None\n",
        "  best_accuracy = 0\n",
        "  # outer k-fold Cross Validation \n",
        "  kfold = KFold(n_splits=10, shuffle=True)\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "      train_subs = torch.utils.data.dataset.Subset(dataset, train_ids)\n",
        "      config = {\n",
        "          'lr_init': [x / 100.0 for x in range(1, 11)],\n",
        "          'num_epochs': list(range(50, 300, 25))\n",
        "      }\n",
        "      all_options = list(product(config['lr_init'], config['num_epochs']))\n",
        "      # select randomly 50 times\n",
        "      for _ in range(50)[:1]:\n",
        "          curr_config = random.choice(all_options)\n",
        "          print(f'choice {curr_config}')\n",
        "          # inner k-fold for hyperparameters optimization\n",
        "          mean_acc = 0\n",
        "          inner_kfold = KFold(n_splits=3, shuffle=True)\n",
        "          for fold, (in_train_ids, in_val_ids) in enumerate(inner_kfold.split(train_subs)):\n",
        "              inner_train = torch.utils.data.dataset.Subset(dataset, in_train_ids)\n",
        "              inner_val = torch.utils.data.dataset.Subset(dataset, in_val_ids)\n",
        "\n",
        "              # Define data loaders for training and validation data in this fold\n",
        "              inner_train_loader = torch.utils.data.DataLoader(\n",
        "                  inner_train,\n",
        "                  batch_size=args.batch_size,\n",
        "                  num_workers=args.num_workers,\n",
        "                  shuffle=True)\n",
        "\n",
        "              inner_val_loader = torch.utils.data.DataLoader(\n",
        "                  inner_val,\n",
        "                  batch_size=args.batch_size,\n",
        "                  shuffle=False,\n",
        "                  num_workers=args.num_workers)\n",
        "\n",
        "              loaders = {'train': inner_train_loader, 'val': inner_val_loader}\n",
        "              acc = find_accuracy_for_specific_hyperparameters(curr_config, model_cfg, loaders)\n",
        "              mean_acc += acc\n",
        "\n",
        "          mean_acc = mean_acc / 3\n",
        "          if best_accuracy < mean_acc:\n",
        "              best_accuracy = mean_acc\n",
        "              best_parameters = curr_config\n",
        "          print(f'best parameters {best_parameters}')\n",
        "\n",
        "\n",
        "  return best_parameters "
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2yYszCkrV3"
      },
      "source": [
        "def download_dataset():\n",
        "  if args.dataset == 'EMNIST':\n",
        "    train_set = ds(path, train=True, download=True, transform=model_cfg.transform_train_gray_scale, split='digits')\n",
        "    test_set = ds(path, train=False, download=True, transform=model_cfg.transform_test_gray_scale, split='digits')\n",
        "  elif args.dataset == 'MNIST' or args.dataset == 'USPS' or args.dataset == 'FashionMNIST' or args.dataset == 'KMNIST' or args.dataset == 'QMNIST':\n",
        "    train_set = ds(path, train=True, download=True, transform=model_cfg.transform_train_gray_scale)\n",
        "    test_set = ds(path, train=False, download=True, transform=model_cfg.transform_test_gray_scale)\n",
        "  elif args.dataset == 'STL10' or args.dataset == 'SVHN':\n",
        "    train_set = ds(path, download= True ,transform=model_cfg.transform_train)\n",
        "  elif args.dataset == 'SEMEION':\n",
        "     train_set = ds(path, download=True, transform=model_cfg.transform_train_gray_scale)\n",
        "  else:# CIFAR10, CIFAR100\n",
        "    train_set = ds(path, train=True, download=True, transform=model_cfg.transform_train)\n",
        "    test_set = ds(path, train=False, download=True, transform=model_cfg.transform_test)\n",
        "\n",
        "  if args.dataset == 'CIFAR100':\n",
        "    num_classes = 100 \n",
        "  else:\n",
        "    num_classes = 10 \n",
        "  if args.dataset == 'STL10' or args.dataset == 'SVHN' or args.dataset == 'SEMEION':\n",
        "    dataset = train_set\n",
        "  else:\n",
        "    dataset = ConcatDataset([train_set, test_set])\n",
        "  return dataset, num_classes"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR84jyx4mL4J"
      },
      "source": [
        "def friedman_test(sgd, swa, improved):\n",
        "  # compare samples\n",
        "  return friedmanchisquare(sgd, swa, improved)\n",
        "\n",
        "def post_hoc_test(sgd, swa, improved):\n",
        "  scores = sgd\n",
        "  scores.extend(swa)\n",
        "  scores.extend(improved)\n",
        "  #create DataFrame to hold data\n",
        "  df = pd.DataFrame({'score': scores,\n",
        "                    'group': np.repeat(['SGD', 'SWA', 'Improved'], repeats=10)}) \n",
        "  # perform Tukey's test\n",
        "  tukey = pairwise_tukeyhsd(endog=df['score'], groups=df['group'], alpha=0.05)\n",
        "  #display results\n",
        "  print(tukey)\n",
        "\n",
        "def significant_test(file_name, dataset_name, metric):\n",
        "  df = pd.read_excel(file_name, header=None).iloc[:,:-1]\n",
        "  df.columns= ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "  sgd = df.loc[(df['Dataset Name'] == dataset_name) & (df['Algorithm Name'] == 'SGD')][metric].values.tolist()\n",
        "  swa = df.loc[(df['Dataset Name'] == dataset_name) & (df['Algorithm Name'] == 'SWA')][metric].values.tolist()\n",
        "  improved =  df.loc[(df['Dataset Name'] == dataset_name) & (df['Algorithm Name'] == 'improved')][metric].values.tolist()\n",
        "  stat, p = friedman_test(sgd, swa, improved)\n",
        "  #print('Statistics=%.3f, p=%.6f' % (stat, p))\n",
        "  # interpret\n",
        "  alpha = 0.05\n",
        "  if p > alpha:\n",
        "    print('Same distributions (fail to reject H0)')\n",
        "  else:\n",
        "    #print('Different distributions (reject H0)')\n",
        "    post_hoc_test(sgd, swa, improved)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BTJUlHxy3Vd"
      },
      "source": [
        "\"\"\"\n",
        "  MAIN\n",
        "\"\"\"\n",
        "#parse args\n",
        "args = get_args()\n",
        "\n",
        "os.makedirs(args.dir, exist_ok=True)\n",
        "with open(os.path.join(args.dir, 'command.sh'), 'w') as f:\n",
        "    f.write(' '.join(sys.argv))\n",
        "    f.write('\\n')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "print('Using model %s' % args.model)\n",
        "model_cfg = VGG16()  \n",
        "\n",
        "if args.swa:\n",
        "  print('Using SWA')\n",
        "  alg_name = 'SWA'\n",
        "elif args.imrpoved:\n",
        "  print('Using SWA Improved')\n",
        "  alg_name = 'Improved'\n",
        "else:\n",
        "  print('Using SGD')\n",
        "  alg_name = 'SGD'\n",
        "\n",
        "#loading dataset from torchvision package \n",
        "print('Loading dataset %s from %s' % (args.dataset, args.data_path))\n",
        "ds = getattr(torchvision.datasets, args.dataset)  \n",
        "path = os.path.join(args.data_path, args.dataset.lower())\n",
        "\n",
        "dataset, num_classes = download_dataset()\n",
        "\n",
        "\n",
        "values = []\n",
        "columns = ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "\n",
        "#hyper parameters optimization\n",
        "best_parameters = hyper_parameters_optimization(dataset)\n",
        "\n",
        "# outer k-fold Cross Validation \n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    train_subs = torch.utils.data.dataset.Subset(dataset, train_ids)\n",
        "    test_subs = torch.utils.data.dataset.Subset(dataset, test_ids)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    train_subs,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=args.num_workers,\n",
        "    shuffle=True)\n",
        "        \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    test_subs,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=args.num_workers,\n",
        "    shuffle=True)\n",
        "\n",
        "    res = evaluate_model(best_parameters[0], best_parameters[1], train_loader, test_loader, num_classes)\n",
        "    value = [args.dataset, alg_name, fold+1, best_parameters] + list(res['metrics'].values())\n",
        "    values.append(value)\n",
        "\n",
        "df_results = pd.DataFrame(data=values, columns = columns)\n",
        "if os.path.isfile('results.xlsx'):\n",
        "  df_results.to_excel('results.xlsx', mode='a', header=False)\n",
        "else:\n",
        "  df_results.to_excel('results.xlsx')\n",
        "print(df_results.to_string())\n",
        "for name in ['CIFAR10', 'CIFAR100', 'EMNIST', 'FashionMNIST', 'KMNIST','MNIST', 'QMNIST', 'STL10', 'USPS', 'SVHN', 'SEMEION']:\n",
        "  print(f'Dataset {name}')\n",
        "  significant_test('datasets.xlsx',name, 'ACC')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}